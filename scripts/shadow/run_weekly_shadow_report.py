#!/usr/bin/env python3
"""
Weekly Shadow Evidence Report.

Phase 2B Operational Script.
Generates comprehensive weekly counterfactual evaluation report.

Contents:
1. Per-symbol: incremental edge (ΔPnL), precision/recall, Sharpe, max DD, turnover
2. Per-regime: same metrics (edge must survive across regimes)
3. Cost decomposition: slippage vs fees vs turnover
4. Confidence sweep: identify best confidence thresholds
5. Drift check: compare vs prior weeks, flag degradation

Output:
- data/reports/weekly_shadow_report_YYYY-MM-DD.json
- docs/reports/weekly_shadow_summary_YYYY-MM-DD.md (optional markdown)

CRITICAL: This is evaluation only. No execution authority.
"""

import json
import logging
import sys
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from bot.rl.counterfactual_evaluator import (
    CounterfactualEvaluator,
    CounterfactualReport,
    CounterfactualTrade,
)
from bot.rl.btc_diagnosis import BTCDiagnosisTool

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
logger = logging.getLogger(__name__)


@dataclass
class WeeklyDriftCheck:
    """Drift comparison vs prior weeks."""
    current_week_sharpe: float = 0.0
    prior_week_sharpe: float = 0.0
    sharpe_change: float = 0.0
    sharpe_degraded: bool = False

    current_week_hit_rate: float = 0.0
    prior_week_hit_rate: float = 0.0
    hit_rate_change: float = 0.0
    hit_rate_degraded: bool = False

    current_week_delta_pnl: float = 0.0
    prior_week_delta_pnl: float = 0.0
    delta_pnl_change: float = 0.0
    edge_degraded: bool = False

    overall_drift_alert: bool = False
    drift_summary: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "sharpe": {
                "current": round(self.current_week_sharpe, 4),
                "prior": round(self.prior_week_sharpe, 4),
                "change": round(self.sharpe_change, 4),
                "degraded": self.sharpe_degraded,
            },
            "hit_rate": {
                "current": round(self.current_week_hit_rate, 4),
                "prior": round(self.prior_week_hit_rate, 4),
                "change": round(self.hit_rate_change, 4),
                "degraded": self.hit_rate_degraded,
            },
            "delta_pnl": {
                "current": round(self.current_week_delta_pnl, 2),
                "prior": round(self.prior_week_delta_pnl, 2),
                "change": round(self.delta_pnl_change, 2),
                "degraded": self.edge_degraded,
            },
            "overall_alert": self.overall_drift_alert,
            "summary": self.drift_summary,
        }


# Data mode constants
DATA_MODE_TEST = "TEST"
DATA_MODE_PAPER_LIVE = "PAPER_LIVE"


@dataclass
class WeeklyShadowReport:
    """Complete weekly shadow evidence report."""
    week_ending: str
    timestamp: str
    week_number: int = 0

    # Data mode: TEST (generated by scripts) or PAPER_LIVE (from live paper trading)
    # Determined by checking the data_mode field in shadow decisions
    data_mode: str = DATA_MODE_TEST  # Default to TEST for safety
    decisions_by_mode: Dict[str, int] = field(default_factory=dict)
    paper_live_decisions: int = 0  # Count of PAPER_LIVE decisions (for gate evaluation)

    # Data collection stats
    total_decisions: int = 0
    decisions_by_symbol: Dict[str, int] = field(default_factory=dict)
    decisions_by_regime: Dict[str, int] = field(default_factory=dict)

    # Counterfactual report (embedded)
    counterfactual_report: Optional[Dict[str, Any]] = None

    # Per-symbol edge analysis
    symbol_edge_analysis: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # Per-regime edge analysis
    regime_edge_analysis: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # Cost decomposition
    cost_decomposition: Dict[str, Any] = field(default_factory=dict)

    # Confidence threshold analysis
    best_confidence_threshold: float = 0.0
    confidence_sweep_results: List[Dict[str, Any]] = field(default_factory=list)

    # Drift check
    drift_check: Optional[WeeklyDriftCheck] = None

    # BTC-specific analysis
    btc_diagnosis: Optional[Dict[str, Any]] = None

    # Summary
    overall_edge_positive: bool = False
    edge_stable_across_regimes: bool = False
    promotion_gate_progress: Dict[str, bool] = field(default_factory=dict)
    alerts: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "week_ending": self.week_ending,
            "timestamp": self.timestamp,
            "week_number": self.week_number,
            "data_mode": self.data_mode,
            "data_collection": {
                "total_decisions": self.total_decisions,
                "decisions_by_mode": self.decisions_by_mode,
                "paper_live_decisions": self.paper_live_decisions,
                "by_symbol": self.decisions_by_symbol,
                "by_regime": self.decisions_by_regime,
            },
            "counterfactual_evaluation": self.counterfactual_report,
            "symbol_edge_analysis": self.symbol_edge_analysis,
            "regime_edge_analysis": self.regime_edge_analysis,
            "cost_decomposition": self.cost_decomposition,
            "confidence_sweep": {
                "best_threshold": self.best_confidence_threshold,
                "results": self.confidence_sweep_results,
            },
            "drift_check": self.drift_check.to_dict() if self.drift_check else None,
            "btc_diagnosis": self.btc_diagnosis,
            "summary": {
                "overall_edge_positive": self.overall_edge_positive,
                "edge_stable_across_regimes": self.edge_stable_across_regimes,
                "promotion_gate_progress": self.promotion_gate_progress,
                "alerts": self.alerts,
                "recommendations": self.recommendations,
            },
        }


class WeeklyShadowReportGenerator:
    """Generates weekly shadow evidence reports."""

    # Phase 2C promotion gate thresholds
    MIN_WEEKS_REQUIRED = 12  # 3 months
    MIN_HIT_RATE = 0.55
    MIN_INCREMENTAL_SHARPE = 0.3
    MAX_INCREMENTAL_DRAWDOWN = 0.05

    def __init__(
        self,
        report_dir: Path = Path("data/reports"),
        docs_dir: Path = Path("docs/reports"),
        shadow_log_path: Path = Path("data/rl/shadow_decisions.jsonl"),
    ):
        self.report_dir = report_dir
        self.docs_dir = docs_dir
        self.shadow_log_path = shadow_log_path

        self.report_dir.mkdir(parents=True, exist_ok=True)
        self.docs_dir.mkdir(parents=True, exist_ok=True)

        self.evaluator = CounterfactualEvaluator(report_dir=report_dir)
        self.btc_tool = BTCDiagnosisTool(output_dir=report_dir)

    def _count_paper_live_weeks(self) -> int:
        """Count number of prior weekly reports with data_mode=PAPER_LIVE.

        Scans existing weekly_shadow_report_*.json files and counts how many
        have data_mode=PAPER_LIVE. TEST weeks are excluded from the count.

        This ensures Gate 1 only counts production data toward the 12-week
        requirement, not test data.
        """
        count = 0
        try:
            for report_file in self.report_dir.glob("weekly_shadow_report_*.json"):
                try:
                    with open(report_file) as f:
                        report_data = json.load(f)

                    # Check data_mode field (default to TEST if missing for backwards compat)
                    data_mode = report_data.get("data_mode", DATA_MODE_TEST)
                    if data_mode == DATA_MODE_PAPER_LIVE:
                        count += 1
                except (json.JSONDecodeError, KeyError):
                    # If file is malformed or missing field, treat as TEST (don't count)
                    continue
        except Exception as e:
            logger.warning(f"Failed to count PAPER_LIVE weeks: {e}")

        return count

    def generate_report(self, week_number: int = 1) -> WeeklyShadowReport:
        """Generate comprehensive weekly shadow report."""
        today = datetime.now()
        week_ending = today.strftime("%Y-%m-%d")

        report = WeeklyShadowReport(
            week_ending=week_ending,
            timestamp=today.isoformat(),
            week_number=week_number,
        )

        # Load shadow data with mode tracking
        trades, mode_counts = self._load_shadow_decisions()

        # Track data_mode statistics
        report.decisions_by_mode = mode_counts
        report.paper_live_decisions = mode_counts.get(DATA_MODE_PAPER_LIVE, 0)

        # Determine overall data_mode for the week
        # Priority: PAPER_LIVE if any exist, otherwise TEST
        if mode_counts.get(DATA_MODE_PAPER_LIVE, 0) > 0:
            report.data_mode = DATA_MODE_PAPER_LIVE
        elif mode_counts.get(DATA_MODE_TEST, 0) > 0:
            report.data_mode = DATA_MODE_TEST
        else:
            report.data_mode = DATA_MODE_TEST  # Default for empty

        if not trades:
            report.alerts.append("No shadow decisions found for this week")
            report.recommendations.append("Verify paper trading is running with shadow mode enabled")
            return report

        report.total_decisions = len(trades)

        # Alert if only TEST data
        if report.data_mode == DATA_MODE_TEST:
            report.alerts.append(
                f"All {report.total_decisions} decisions are TEST data - "
                "this week will NOT count toward Phase 2C promotion gates"
            )

        # Count by symbol and regime
        for trade in trades:
            symbol = trade.symbol
            regime = trade.regime
            report.decisions_by_symbol[symbol] = report.decisions_by_symbol.get(symbol, 0) + 1
            report.decisions_by_regime[regime] = report.decisions_by_regime.get(regime, 0) + 1

        # Run counterfactual evaluation
        cf_report = self.evaluator.evaluate(trades, weeks=1)
        report.counterfactual_report = cf_report.to_dict()

        # Per-symbol edge analysis
        self._analyze_symbol_edges(report, cf_report, trades)

        # Per-regime edge analysis
        self._analyze_regime_edges(report, cf_report)

        # Cost decomposition
        self._analyze_costs(report, cf_report)

        # Confidence sweep analysis
        self._analyze_confidence_sweep(report, cf_report)

        # Drift check (compare to prior week)
        self._check_drift(report, week_number)

        # BTC-specific diagnosis
        self._run_btc_diagnosis(report, trades)

        # Check promotion gates
        self._check_promotion_gates(report, cf_report, week_number)

        # Generate summary
        self._generate_summary(report, cf_report)

        return report

    def _load_shadow_decisions(self) -> Tuple[List[CounterfactualTrade], Dict[str, int]]:
        """Load shadow decisions from the past week.

        Returns:
            Tuple of (trades, mode_counts) where mode_counts tracks
            the number of decisions by data_mode (TEST vs PAPER_LIVE).
        """
        trades = []
        mode_counts = {DATA_MODE_TEST: 0, DATA_MODE_PAPER_LIVE: 0}
        week_ago = datetime.now() - timedelta(days=7)

        if not self.shadow_log_path.exists():
            logger.warning(f"Shadow log not found: {self.shadow_log_path}")
            return trades, mode_counts

        try:
            with open(self.shadow_log_path) as f:
                for line in f:
                    try:
                        entry = json.loads(line)
                        timestamp_str = entry.get("timestamp", "")
                        if not timestamp_str:
                            continue

                        timestamp = datetime.fromisoformat(timestamp_str)
                        if timestamp < week_ago:
                            continue

                        # Track data_mode (default to TEST if missing for backwards compat)
                        data_mode = entry.get("data_mode", DATA_MODE_TEST)
                        mode_counts[data_mode] = mode_counts.get(data_mode, 0) + 1

                        # Convert to CounterfactualTrade
                        rl_rec = entry.get("rl_recommendation", {})
                        actual = entry.get("actual_decision", {})
                        outcome = entry.get("outcome", {})
                        costs = entry.get("costs", {})
                        gate = entry.get("gate_decision", {})

                        trade = CounterfactualTrade(
                            timestamp=timestamp,
                            symbol=entry.get("symbol", ""),
                            regime=entry.get("regime_context", {}).get("regime", "unknown"),
                            rl_action=rl_rec.get("action", "hold"),
                            rl_confidence=rl_rec.get("confidence", 0.0),
                            rl_primary_agent=rl_rec.get("primary_agent", ""),
                            actual_action=actual.get("action", "hold"),
                            actual_pnl=outcome.get("pnl", 0.0),
                            slippage_cost=costs.get("slippage", 0.0),
                            fee_cost=costs.get("fees", 0.0),
                            spread_cost=costs.get("spread", 0.0),
                            gate_approved=gate.get("approved", True),
                            gate_rejection_reason=gate.get("rejection_reason", ""),
                            followed_rl=(
                                rl_rec.get("action", "hold") == actual.get("action", "hold")
                            ),
                            counterfactual_pnl=outcome.get("pnl", 0.0),  # Simplified
                        )
                        trades.append(trade)

                    except (json.JSONDecodeError, KeyError) as e:
                        logger.debug(f"Skipping malformed entry: {e}")
                        continue

        except Exception as e:
            logger.error(f"Failed to load shadow decisions: {e}")

        logger.info(
            f"Loaded {len(trades)} shadow decisions from past week "
            f"(TEST: {mode_counts.get(DATA_MODE_TEST, 0)}, "
            f"PAPER_LIVE: {mode_counts.get(DATA_MODE_PAPER_LIVE, 0)})"
        )
        return trades, mode_counts

    def _analyze_symbol_edges(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
        trades: List[CounterfactualTrade],
    ):
        """Analyze edge per symbol."""
        for symbol in report.decisions_by_symbol:
            symbol_trades = [t for t in trades if t.symbol == symbol]
            if not symbol_trades:
                continue

            # Calculate metrics
            total_actual = sum(t.actual_pnl for t in symbol_trades)
            total_cf = sum(t.counterfactual_pnl for t in symbol_trades)
            followed = sum(1 for t in symbol_trades if t.followed_rl)
            correct = sum(1 for t in symbol_trades if t.followed_rl and t.actual_pnl > 0)

            hit_rate = correct / len(symbol_trades) if symbol_trades else 0
            delta_pnl = total_cf - total_actual
            turnover = len(symbol_trades)

            # Calculate simple Sharpe (would need more data for proper calculation)
            pnls = [t.actual_pnl for t in symbol_trades]
            avg_pnl = sum(pnls) / len(pnls) if pnls else 0
            std_pnl = (sum((p - avg_pnl) ** 2 for p in pnls) / len(pnls)) ** 0.5 if pnls else 1
            sharpe = avg_pnl / std_pnl if std_pnl > 0 else 0

            # Max drawdown (simplified)
            cumulative = 0
            peak = 0
            max_dd = 0
            for t in symbol_trades:
                cumulative += t.actual_pnl
                if cumulative > peak:
                    peak = cumulative
                dd = (peak - cumulative) / peak if peak > 0 else 0
                if dd > max_dd:
                    max_dd = dd

            report.symbol_edge_analysis[symbol] = {
                "decisions": len(symbol_trades),
                "delta_pnl": round(delta_pnl, 2),
                "hit_rate": round(hit_rate, 4),
                "sharpe": round(sharpe, 4),
                "max_drawdown": round(max_dd, 4),
                "turnover": turnover,
                "edge_positive": delta_pnl > 0,
            }

    def _analyze_regime_edges(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
    ):
        """Analyze edge per regime."""
        for regime, metrics in cf_report.by_regime.items():
            report.regime_edge_analysis[regime] = {
                "decisions": metrics.total_decisions,
                "delta_pnl": round(metrics.delta_pnl, 2),
                "hit_rate": round(metrics.rl_hit_rate, 4),
                "avg_confidence": round(metrics.avg_rl_confidence, 4),
                "edge_positive": metrics.delta_pnl > 0,
            }

        # Check if edge survives across regimes
        positive_regimes = sum(
            1 for r in report.regime_edge_analysis.values()
            if r.get("edge_positive", False)
        )
        total_regimes = len(report.regime_edge_analysis)
        report.edge_stable_across_regimes = (
            positive_regimes >= total_regimes * 0.6 if total_regimes > 0 else False
        )

    def _analyze_costs(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
    ):
        """Analyze cost decomposition."""
        costs = cf_report.costs
        report.cost_decomposition = {
            "total_slippage": round(costs.total_slippage, 2),
            "total_fees": round(costs.total_fees, 2),
            "total_spread": round(costs.total_spread, 2),
            "total_cost": round(costs.total_cost, 2),
            "avg_cost_per_trade": round(costs.avg_cost_per_trade, 4),
            "cost_as_pct_of_pnl": round(costs.cost_as_pct_of_pnl, 4),
        }

        # Alert if costs are eating into edge
        if costs.cost_as_pct_of_pnl > 0.5:  # Costs > 50% of PnL
            report.alerts.append(
                f"High cost ratio: costs are {costs.cost_as_pct_of_pnl:.1%} of PnL"
            )
            report.recommendations.append(
                "Consider reducing turnover or improving execution"
            )

    def _analyze_confidence_sweep(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
    ):
        """Analyze confidence threshold sweep."""
        best_threshold = 0.5
        best_sharpe = -999

        for result in cf_report.threshold_analysis:
            report.confidence_sweep_results.append({
                "threshold": result.threshold,
                "trades_above": result.trades_above_threshold,
                "avg_pnl_above": round(result.avg_pnl_above, 2),
                "avg_pnl_below": round(result.avg_pnl_below, 2),
                "incremental_sharpe": round(result.incremental_sharpe, 4),
                "hit_rate_above": round(result.hit_rate_above, 4),
            })

            if result.incremental_sharpe > best_sharpe:
                best_sharpe = result.incremental_sharpe
                best_threshold = result.threshold

        report.best_confidence_threshold = best_threshold

        if best_threshold > 0.7:
            report.recommendations.append(
                f"Best confidence threshold is {best_threshold:.1%} - "
                f"consider only following RL above this level"
            )

    def _check_drift(self, report: WeeklyShadowReport, week_number: int):
        """Check for drift vs prior week."""
        drift = WeeklyDriftCheck()

        # Try to load prior week's report
        prior_week_date = (
            datetime.now() - timedelta(days=7)
        ).strftime("%Y-%m-%d")
        prior_report_path = self.report_dir / f"weekly_shadow_report_{prior_week_date}.json"

        if prior_report_path.exists():
            try:
                with open(prior_report_path) as f:
                    prior = json.load(f)

                prior_cf = prior.get("counterfactual_evaluation", {})

                # Extract prior metrics
                drift.prior_week_sharpe = prior_cf.get("pnl_comparison", {}).get(
                    "incremental_sharpe", 0
                )
                drift.prior_week_hit_rate = prior_cf.get("overall", {}).get(
                    "rl_hit_rate", 0
                )
                drift.prior_week_delta_pnl = prior_cf.get("pnl_comparison", {}).get(
                    "delta_pnl", 0
                )

                # Current metrics
                if report.counterfactual_report:
                    drift.current_week_sharpe = report.counterfactual_report.get(
                        "pnl_comparison", {}
                    ).get("incremental_sharpe", 0)
                    drift.current_week_hit_rate = report.counterfactual_report.get(
                        "overall", {}
                    ).get("rl_hit_rate", 0)
                    drift.current_week_delta_pnl = report.counterfactual_report.get(
                        "pnl_comparison", {}
                    ).get("delta_pnl", 0)

                # Calculate changes
                drift.sharpe_change = drift.current_week_sharpe - drift.prior_week_sharpe
                drift.hit_rate_change = drift.current_week_hit_rate - drift.prior_week_hit_rate
                drift.delta_pnl_change = drift.current_week_delta_pnl - drift.prior_week_delta_pnl

                # Check for degradation
                drift.sharpe_degraded = drift.sharpe_change < -0.2
                drift.hit_rate_degraded = drift.hit_rate_change < -0.05
                drift.edge_degraded = drift.delta_pnl_change < -100  # $100 drop

                drift.overall_drift_alert = (
                    drift.sharpe_degraded or
                    drift.hit_rate_degraded or
                    drift.edge_degraded
                )

                if drift.overall_drift_alert:
                    drift.drift_summary = "Performance degradation detected vs prior week"
                    report.alerts.append(drift.drift_summary)
                else:
                    drift.drift_summary = "Performance stable vs prior week"

            except Exception as e:
                drift.drift_summary = f"Could not load prior week data: {e}"

        else:
            drift.drift_summary = "No prior week data available for comparison"

        report.drift_check = drift

    def _run_btc_diagnosis(
        self,
        report: WeeklyShadowReport,
        trades: List[CounterfactualTrade],
    ):
        """Run BTC-specific diagnosis."""
        btc_trades = [t for t in trades if "BTC" in t.symbol]
        if not btc_trades:
            return

        # Build simplified BTC results
        btc_results = {
            "period_days": 7,
            "gross_return_pct": 0,
            "net_return_pct": 0,
            "total_trades": len(btc_trades),
            "win_rate": sum(1 for t in btc_trades if t.actual_pnl > 0) / len(btc_trades),
            "total_slippage_cost": sum(t.slippage_cost for t in btc_trades),
            "total_spread_cost": sum(t.spread_cost for t in btc_trades),
            "total_fee_cost": sum(t.fee_cost for t in btc_trades),
            "avg_trade_value": 1000,
            "avg_position_size": 1000,
            "avg_holding_time_hours": 2.0,
        }

        try:
            btc_report = self.btc_tool.analyze_from_backtest_results(btc_results)
            report.btc_diagnosis = {
                "primary_cause": btc_report.primary_cause,
                "estimated_recoverable_pct": btc_report.estimated_recoverable_pct,
                "recommendations": [
                    {
                        "category": r.category,
                        "priority": r.priority,
                        "issue": r.issue,
                        "recommendation": r.recommendation,
                    }
                    for r in btc_report.recommendations[:3]  # Top 3
                ],
            }

            if btc_report.estimated_recoverable_pct > 10:
                report.recommendations.append(
                    f"BTC: {btc_report.primary_cause} - "
                    f"~{btc_report.estimated_recoverable_pct:.0f}% return recoverable"
                )

        except Exception as e:
            logger.warning(f"BTC diagnosis failed: {e}")

    def _check_promotion_gates(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
        week_number: int,
    ):
        """Check Phase 2C promotion gate progress."""
        gates = {}

        # Gate 1: 3+ months (12 weeks) of PAPER_LIVE data
        # TEST data does NOT count toward this requirement
        weeks_with_paper_live = self._count_paper_live_weeks()
        is_paper_live_week = report.data_mode == DATA_MODE_PAPER_LIVE

        gates["weeks_collected"] = {
            "required": self.MIN_WEEKS_REQUIRED,
            "current": weeks_with_paper_live,
            "current_week_data_mode": report.data_mode,
            "counts_toward_gate": is_paper_live_week,
            "met": weeks_with_paper_live >= self.MIN_WEEKS_REQUIRED,
            "note": (
                "Only PAPER_LIVE weeks count toward this requirement. "
                f"TEST weeks are excluded."
            ),
        }

        if not is_paper_live_week:
            report.alerts.append(
                f"This week's data_mode is {report.data_mode} - "
                "does NOT count toward Gate 1 (weeks collected)"
            )

        # Gate 2: Positive net-of-cost edge
        gates["positive_edge"] = {
            "required": True,
            "current": cf_report.delta_pnl > 0,
            "met": cf_report.delta_pnl > 0,
        }

        # Gate 3: Hit rate threshold
        gates["hit_rate"] = {
            "required": self.MIN_HIT_RATE,
            "current": cf_report.rl_hit_rate,
            "met": cf_report.rl_hit_rate >= self.MIN_HIT_RATE,
        }

        # Gate 4: Incremental Sharpe
        gates["incremental_sharpe"] = {
            "required": self.MIN_INCREMENTAL_SHARPE,
            "current": cf_report.incremental_sharpe,
            "met": cf_report.incremental_sharpe >= self.MIN_INCREMENTAL_SHARPE,
        }

        # Gate 5: Max drawdown
        gates["max_drawdown"] = {
            "required": f"<{self.MAX_INCREMENTAL_DRAWDOWN}",
            "current": cf_report.counterfactual_max_drawdown,
            "met": cf_report.counterfactual_max_drawdown < self.MAX_INCREMENTAL_DRAWDOWN,
        }

        # Gate 6: BTC mitigation
        gates["btc_mitigation"] = {
            "required": "Turnover reduction validated",
            "current": "In progress" if report.btc_diagnosis else "Not started",
            "met": False,  # Requires manual validation
        }

        # Gate 7: Human sign-off
        gates["human_signoff"] = {
            "required": True,
            "current": False,
            "met": False,
        }

        report.promotion_gate_progress = gates

        # Summarize gate status
        gates_met = sum(1 for g in gates.values() if g.get("met", False))
        total_gates = len(gates)

        report.recommendations.append(
            f"Phase 2C promotion gates: {gates_met}/{total_gates} met"
        )

        if gates_met < total_gates:
            report.alerts.append("Phase 2C remains BLOCKED - not all gates met")

    def _generate_summary(
        self,
        report: WeeklyShadowReport,
        cf_report: CounterfactualReport,
    ):
        """Generate final summary."""
        report.overall_edge_positive = cf_report.delta_pnl > 0

        if report.overall_edge_positive:
            report.recommendations.append(
                f"Positive RL edge detected: Δ${cf_report.delta_pnl:.2f}"
            )
        else:
            report.alerts.append(
                f"Negative RL edge: Δ${cf_report.delta_pnl:.2f} - RL recommendations "
                f"would have performed worse"
            )

        if not report.edge_stable_across_regimes:
            report.alerts.append(
                "Edge not stable across regimes - RL may be overfitting to specific conditions"
            )

    def save_report(self, report: WeeklyShadowReport) -> Tuple[Path, Path]:
        """Save weekly report as JSON and markdown."""
        # JSON report
        json_filename = f"weekly_shadow_report_{report.week_ending}.json"
        json_path = self.report_dir / json_filename
        with open(json_path, "w") as f:
            json.dump(report.to_dict(), f, indent=2)
        logger.info(f"JSON report saved: {json_path}")

        # Markdown summary
        md_filename = f"weekly_shadow_summary_{report.week_ending}.md"
        md_path = self.docs_dir / md_filename
        md_content = self._generate_markdown(report)
        with open(md_path, "w") as f:
            f.write(md_content)
        logger.info(f"Markdown summary saved: {md_path}")

        return json_path, md_path

    def _generate_markdown(self, report: WeeklyShadowReport) -> str:
        """Generate markdown summary."""
        lines = [
            f"# Weekly Shadow Report - Week {report.week_number}",
            "",
            f"**Week Ending:** {report.week_ending}",
            f"**Generated:** {report.timestamp}",
            "",
            "## Summary",
            "",
            f"- **Total Decisions:** {report.total_decisions}",
            f"- **Overall Edge Positive:** {'Yes' if report.overall_edge_positive else 'No'}",
            f"- **Edge Stable Across Regimes:** {'Yes' if report.edge_stable_across_regimes else 'No'}",
            "",
        ]

        # Alerts
        if report.alerts:
            lines.extend([
                "## Alerts",
                "",
            ])
            for alert in report.alerts:
                lines.append(f"- ⚠️ {alert}")
            lines.append("")

        # Symbol Analysis
        if report.symbol_edge_analysis:
            lines.extend([
                "## Per-Symbol Edge Analysis",
                "",
                "| Symbol | Decisions | ΔPnL | Hit Rate | Sharpe | Max DD | Edge+ |",
                "|--------|-----------|------|----------|--------|--------|-------|",
            ])
            for symbol, metrics in report.symbol_edge_analysis.items():
                lines.append(
                    f"| {symbol} | {metrics['decisions']} | "
                    f"${metrics['delta_pnl']:.2f} | {metrics['hit_rate']:.1%} | "
                    f"{metrics['sharpe']:.2f} | {metrics['max_drawdown']:.1%} | "
                    f"{'✓' if metrics['edge_positive'] else '✗'} |"
                )
            lines.append("")

        # Regime Analysis
        if report.regime_edge_analysis:
            lines.extend([
                "## Per-Regime Edge Analysis",
                "",
                "| Regime | Decisions | ΔPnL | Hit Rate | Edge+ |",
                "|--------|-----------|------|----------|-------|",
            ])
            for regime, metrics in report.regime_edge_analysis.items():
                lines.append(
                    f"| {regime} | {metrics['decisions']} | "
                    f"${metrics['delta_pnl']:.2f} | {metrics['hit_rate']:.1%} | "
                    f"{'✓' if metrics['edge_positive'] else '✗'} |"
                )
            lines.append("")

        # Cost Decomposition
        if report.cost_decomposition:
            costs = report.cost_decomposition
            lines.extend([
                "## Cost Decomposition",
                "",
                f"- **Total Slippage:** ${costs.get('total_slippage', 0):.2f}",
                f"- **Total Fees:** ${costs.get('total_fees', 0):.2f}",
                f"- **Total Spread:** ${costs.get('total_spread', 0):.2f}",
                f"- **Avg Cost/Trade:** ${costs.get('avg_cost_per_trade', 0):.4f}",
                f"- **Cost as % of PnL:** {costs.get('cost_as_pct_of_pnl', 0):.1%}",
                "",
            ])

        # Confidence Threshold
        lines.extend([
            "## Best Confidence Threshold",
            "",
            f"**Recommended Threshold:** {report.best_confidence_threshold:.1%}",
            "",
        ])

        # Promotion Gates
        if report.promotion_gate_progress:
            lines.extend([
                "## Phase 2C Promotion Gate Progress",
                "",
                "| Gate | Required | Current | Met |",
                "|------|----------|---------|-----|",
            ])
            for gate, status in report.promotion_gate_progress.items():
                met = "✓" if status.get("met", False) else "✗"
                lines.append(
                    f"| {gate} | {status.get('required', '-')} | "
                    f"{status.get('current', '-')} | {met} |"
                )
            lines.append("")

        # Recommendations
        if report.recommendations:
            lines.extend([
                "## Recommendations",
                "",
            ])
            for rec in report.recommendations:
                lines.append(f"- {rec}")
            lines.append("")

        return "\n".join(lines)


def main():
    """Run weekly shadow evidence report."""
    import argparse

    parser = argparse.ArgumentParser(description="Generate weekly shadow evidence report")
    parser.add_argument("--week", type=int, default=1, help="Week number in the program")
    args = parser.parse_args()

    logger.info("=" * 60)
    logger.info(f"Weekly Shadow Evidence Report - Week {args.week}")
    logger.info("=" * 60)

    generator = WeeklyShadowReportGenerator()
    report = generator.generate_report(week_number=args.week)

    # Save reports
    json_path, md_path = generator.save_report(report)

    # Print summary
    print("\n" + "=" * 60)
    print(f"WEEKLY SHADOW REPORT - Week {args.week}")
    print(f"Week ending: {report.week_ending}")
    print("=" * 60)

    print(f"\nTotal Decisions: {report.total_decisions}")
    print(f"Overall Edge Positive: {report.overall_edge_positive}")
    print(f"Edge Stable Across Regimes: {report.edge_stable_across_regimes}")

    if report.alerts:
        print(f"\n⚠️  ALERTS ({len(report.alerts)}):")
        for alert in report.alerts:
            print(f"  - {alert}")

    # Print promotion gate summary
    if report.promotion_gate_progress:
        gates_met = sum(
            1 for g in report.promotion_gate_progress.values()
            if g.get("met", False)
        )
        total_gates = len(report.promotion_gate_progress)
        print(f"\nPhase 2C Promotion Gates: {gates_met}/{total_gates} met")

        if gates_met < total_gates:
            print("Status: BLOCKED")
        else:
            print("Status: Ready for human review")

    print(f"\nReports saved:")
    print(f"  - JSON: {json_path}")
    print(f"  - Markdown: {md_path}")
    print("=" * 60)


if __name__ == "__main__":
    main()
